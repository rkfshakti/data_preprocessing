{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "564a0969",
   "metadata": {},
   "source": [
    "# Preprocessing demo\n",
    "This notebook demonstrates the cleaning and tokenization steps used in the demo project. It uses the local library in `src/` so start the kernel from the project root."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b171fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a small dataset (if missing) and load modules\n",
    "from generate_data import generate\n",
    "from src.preprocess import clean_text, tokenize_and_lemmatize\n",
    "from pathlib import Path\n",
    "DATA = Path('data') / 'synthetic_texts.csv'\n",
    "if not DATA.exists():\n",
    "    generate(200)\n",
    "print('Data available at', DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13fe5aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    " \"cells\": [\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Text Preprocessing Demo\\n\",\n",
    "    \"\\n\",\n",
    "    \"This notebook demonstrates comprehensive text preprocessing techniques including cleaning, tokenization, lemmatization, and feature engineering.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## Setup and Data Loading\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"import pandas as pd\\n\",\n",
    "    \"import numpy as np\\n\",\n",
    "    \"import matplotlib.pyplot as plt\\n\",\n",
    "    \"import seaborn as sns\\n\",\n",
    "    \"from pathlib import Path\\n\",\n",
    "    \"\\n\",\n",
    "    \"from src.preprocess import clean_text, tokenize_and_lemmatize, preprocess_df\\n\",\n",
    "    \"from generate_data import generate\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Generate data if needed\\n\",\n",
    "    \"DATA = Path('data') / 'synthetic_texts.csv'\\n\",\n",
    "    \"if not DATA.exists():\\n\",\n",
    "    \"    generate(200)\\n\",\n",
    "    \"\\n\",\n",
    "    \"print('Data available at', DATA)\\n\",\n",
    "    \"df = pd.read_csv(DATA)\\n\",\n",
    "    \"print(f\\\"Dataset shape: {df.shape}\\\")\\n\",\n",
    "    \"print(\\\"Label distribution:\\\")\\n\",\n",
    "    \"print(df['label'].value_counts())\\n\",\n",
    "    \"df.head()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## Text Cleaning Examples\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Test cleaning on various types of text\\n\",\n",
    "    \"samples = [\\n\",\n",
    "    \"    '<p>Hello!!! Visit https://example.com for more üòä</p>',\\n\",\n",
    "    \"    'Contact us at info@example.com or call +1 (555) 123-4567',\\n\",\n",
    "    \"    'Repeated words words words words',\\n\",\n",
    "    \"    'SPECIAL characters: !@#$%^&*()_+',\\n\",\n",
    "    \"    'Mixed CASE Text with Numbers 123',\\n\",\n",
    "    \"    'HTML tags: <div>content</div> and <script>alert()</script>',\\n\",\n",
    "    \"    'Emoji test: üòä üëç üöÄ',\\n\",\n",
    "    \"    'URLs: http://example.com and https://secure.site/path?query=value'\\n\",\n",
    "    \"]\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"Text Cleaning Demonstration:\\\")\\n\",\n",
    "    \"print(\\\"=\\\" * 50)\\n\",\n",
    "    \"for i, s in enumerate(samples, 1):\\n\",\n",
    "    \"    print(f\\\"\\\\nExample {i}:\\\")\\n\",\n",
    "    \"    print(f\\\"RAW   : {s}\\\")\\n\",\n",
    "    \"    print(f\\\"CLEAN : {clean_text(s)}\\\")\\n\",\n",
    "    \"    print(\\\"-\\\" * 30)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## Tokenization and Lemmatization\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Show detailed tokenization process\\n\",\n",
    "    \"print(\\\"Tokenization and Lemmatization Process:\\\")\\n\",\n",
    "    \"print(\\\"=\\\" * 60)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Select sample texts from different domains\\n\",\n",
    "    \"sample_rows = df.sample(5, random_state=42)\\n\",\n",
    "    \"\\n\",\n",
    "    \"for idx, row in sample_rows.iterrows():\\n\",\n",
    "    \"    print(f\\\"\\\\nSample {idx} (Domain: {row['domain']}, Label: {row['label']}):\\\")\\n\",\n",
    "    \"    print(f\\\"Original: {row['text']}\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    cleaned = clean_text(row['text'])\\n\",\n",
    "    \"    print(f\\\"Cleaned : {cleaned}\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    tokens = tokenize_and_lemmatize(cleaned)\\n\",\n",
    "    \"    print(f\\\"Tokens  : {tokens}\\\")\\n\",\n",
    "    \"    print(f\\\"Token count: {len(tokens)}\\\")\\n\",\n",
    "    \"    print(\\\"-\\\" * 50)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## Batch Preprocessing\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Apply preprocessing to entire dataset\\n\",\n",
    "    \"print(\\\"Batch Preprocessing Results:\\\")\\n\",\n",
    "    \"print(\\\"=\\\" * 40)\\n\",\n",
    "    \"\\n\",\n",
    "    \"preprocessed_df = preprocess_df(df)\\n\",\n",
    "    \"print(f\\\"Preprocessed dataset shape: {preprocessed_df.shape}\\\")\\n\",\n",
    "    \"print(\\\"\\\\nFirst 5 rows of preprocessed data:\\\")\\n\",\n",
    "    \"preprocessed_df.head()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## Preprocessing Statistics\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Analyze preprocessing results\\n\",\n",
    "    \"print(\\\"Preprocessing Statistics:\\\")\\n\",\n",
    "    \"print(\\\"=\\\" * 30)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Calculate text length statistics\\n\",\n",
    "    \"preprocessed_df['original_length'] = df['text'].str.split().str.len()\\n\",\n",
    "    \"preprocessed_df['processed_length'] = preprocessed_df['tokens'].str.len()\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"Average original text length: {preprocessed_df['original_length'].mean():.1f} words\\\")\\n\",\n",
    "    \"print(f\\\"Average processed text length: {preprocessed_df['processed_length'].mean():.1f} tokens\\\")\\n\",\n",
    "    \"print(f\\\"Reduction ratio: {(1 - preprocessed_df['processed_length'].mean() / preprocessed_df['original_length'].mean()) * 100:.1f}%\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Show length distribution\\n\",\n",
    "    \"plt.figure(figsize=(12, 5))\\n\",\n",
    "    \"\\n\",\n",
    "    \"plt.subplot(1, 2, 1)\\n\",\n",
    "    \"plt.hist(preprocessed_df['original_length'], bins=20, alpha=0.7, label='Original')\\n\",\n",
    "    \"plt.hist(preprocessed_df['processed_length'], bins=20, alpha=0.7, label='Processed')\\n\",\n",
    "    \"plt.xlabel('Text Length (words/tokens)')\\n\",\n",
    "    \"plt.ylabel('Frequency')\\n\",\n",
    "    \"plt.title('Text Length Distribution')\\n\",\n",
    "    \"plt.legend()\\n\",\n",
    "    \"\\n\",\n",
    "    \"plt.subplot(1, 2, 2)\\n\",\n",
    "    \"reduction_ratio = (preprocessed_df['original_length'] - preprocessed_df['processed_length']) / preprocessed_df['original_length']\\n\",\n",
    "    \"plt.hist(reduction_ratio, bins=20, alpha=0.7)\\n\",\n",
    "    \"plt.xlabel('Reduction Ratio')\\n\",\n",
    "    \"plt.ylabel('Frequency')\\n\",\n",
    "    \"plt.title('Text Reduction Ratio Distribution')\\n\",\n",
    "    \"plt.tight_layout()\\n\",\n",
    "    \"plt.show()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## Vocabulary Analysis\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Analyze vocabulary before and after preprocessing\\n\",\n",
    "    \"print(\\\"Vocabulary Analysis:\\\")\\n\",\n",
    "    \"print(\\\"=\\\" * 25)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Original vocabulary\\n\",\n",
    "    \"original_words = []\\n\",\n",
    "    \"for text in df['text']:\\n\",\n",
    "    \"    words = text.lower().split()\\n\",\n",
    "    \"    original_words.extend(words)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Processed vocabulary\\n\",\n",
    "    \"processed_words = []\\n\",\n",
    "    \"for tokens in preprocessed_df['tokens']:\\n\",\n",
    "    \"    processed_words.extend(tokens)\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"Original vocabulary size: {len(set(original_words))}\\\")\\n\",\n",
    "    \"print(f\\\"Processed vocabulary size: {len(set(processed_words))}\\\")\\n\",\n",
    "    \"print(f\\\"Vocabulary reduction: {(1 - len(set(processed_words)) / len(set(original_words))) * 100:.1f}%\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Show most common words\\n\",\n",
    "    \"from collections import Counter\\n\",\n",
    "    \"\\n\",\n",
    "    \"original_counter = Counter(original_words)\\n\",\n",
    "    \"processed_counter = Counter(processed_words)\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"\\\\nTop 10 original words:\\\")\\n\",\n",
    "    \"for word, count in original_counter.most_common(10):\\n\",\n",
    "    \"    print(f\\\"  {word}: {count}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"\\\\nTop 10 processed words:\\\")\\n\",\n",
    "    \"for word, count in processed_counter.most_common(10):\\n\",\n",
    "    \"    print(f\\\"  {word}: {count}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## Domain-Specific Preprocessing Analysis\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Analyze preprocessing effects by domain\\n\",\n",
    "    \"print(\\\"Domain-Specific Preprocessing Analysis:\\\")\\n\",\n",
    "    \"print(\\\"=\\\" * 45)\\n\",\n",
    "    \"\\n\",\n",
    "    \"domain_stats = preprocessed_df.groupby('domain').agg({\\n\",\n",
    "    \"    'original_length': 'mean',\\n\",\n",
    "    \"    'processed_length': 'mean',\\n\",\n",
    "    \"    'tokens': 'count'\\n\",\n",
    "    \"}).round(1)\\n\",\n",
    "    \"\\n\",\n",
    "    \"domain_stats['reduction_ratio'] = ((domain_stats['original_length'] - domain_stats['processed_length']) / \\n\",\n",
    "    \"                                 domain_stats['original_length'] * 100).round(1)\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"\\\\nPreprocessing statistics by domain:\\\")\\n\",\n",
    "    \"print(domain_stats)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Visualize domain differences\\n\",\n",
    "    \"plt.figure(figsize=(10, 6))\\n\",\n",
    "    \"domains = domain_stats.index\\n\",\n",
    "    \"x = np.arange(len(domains))\\n\",\n",
    "    \"width = 0.35\\n\",\n",
    "    \"\\n\",\n",
    "    \"plt.bar(x - width/2, domain_stats['original_length'], width, label='Original')\\n\",\n",
    "    \"plt.bar(x + width/2, domain_stats['processed_length'], width, label='Processed')\\n\",\n",
    "    \"plt.xlabel('Domain')\\n\",\n",
    "    \"plt.ylabel('Average Text Length')\\n\",\n",
    "    \"plt.title('Text Length by Domain (Before/After Preprocessing)')\\n\",\n",
    "    \"plt.xticks(x, domains)\\n\",\n",
    "    \"plt.legend()\\n\",\n",
    "    \"plt.show()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## Custom Preprocessing Examples\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Demonstrate custom preprocessing options\\n\",\n",
    "    \"print(\\\"Custom Preprocessing Options:\\\")\\n\",\n",
    "    \"print(\\\"=\\\" * 35)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Example of custom cleaning function\\n\",\n",
    "    \"def custom_clean_text(text, remove_numbers=True, remove_special_chars=True):\\n\",\n",
    "    \"    \\\"\\\"\\\"Custom text cleaning with configurable options\\\"\\\"\\\"\\n\",\n",
    "    \"    cleaned = clean_text(text)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    if remove_numbers:\\n\",\n",
    "    \"        # Remove numbers\\n\",\n",
    "    \"        import re\\n\",\n",
    "    \"        cleaned = re.sub(r'\\\\d+', '', cleaned)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    if remove_special_chars:\\n\",\n",
    "    \"        # Remove special characters\\n\",\n",
    "    \"        cleaned = re.sub(r'[^\\\\w\\\\s]', '', cleaned)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    return cleaned.strip()\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Test custom preprocessing\\n\",\n",
    "    \"test_texts = [\\n\",\n",
    "    \"    \\\"Product ID: ABC123, Price: $99.99, Rating: 4.5/5\\\",\\n\",\n",
    "    \"    \\\"Special! Buy now & save 50%!!!\\\",\\n\",\n",
    "    \"    \\\"Contact: john.doe@example.com, Phone: (555) 123-4567\\\"\\n\",\n",
    "    \"]\\n\",\n",
    "    \"\\n\",\n",
    "    \"for text in test_texts:\\n\",\n",
    "    \"    print(f\\\"\\\\nOriginal: {text}\\\")\\n\",\n",
    "    \"    print(f\\\"Standard: {clean_text(text)}\\\")\\n\",\n",
    "    \"    print(f\\\"Custom  : {custom_clean_text(text, remove_numbers=True, remove_special_chars=True)}\\\")\\n\",\n",
    "    \"    print(\\\"-\\\" * 50)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## Preprocessing Pipeline Summary\\n\",\n",
    "    \"\\n\",\n",
    "    \"This notebook demonstrates:\\n\",\n",
    "    \"1. Text cleaning (HTML removal, URL/email removal, special character handling)\\n\",\n",
    "    \"2. Tokenization and lemmatization\\n\",\n",
    "    \"3. Batch processing of datasets\\n\",\n",
    "    \"4. Statistical analysis of preprocessing effects\\n\",\n",
    "    \"5. Vocabulary reduction analysis\\n\",\n",
    "    \"6. Domain-specific preprocessing characteristics\\n\",\n",
    "    \"7. Custom preprocessing options\\n\",\n",
    "    \"\\n\",\n",
    "    \"Key insights:\\n\",\n",
    "    \"- Preprocessing significantly reduces text length and vocabulary size\\n\",\n",
    "    \"- Different domains may require different preprocessing strategies\\n\",\n",
    "    \"- Custom preprocessing can be tailored to specific use cases\\n\",\n",
    "    \"- Proper preprocessing improves downstream NLP task performance\"\n",
    "   ]\n",
    "  }\n",
    " ],\n",
    " \"metadata\": {\n",
    "  \"kernelspec\": {\n",
    "   \"display_name\": \"Python 3\",\n",
    "   \"language\": \"python\",\n",
    "   \"name\": \"python3\"\n",
    "  },\n",
    "  \"language_info\": {\n",
    "   \"name\": \"python\",\n",
    "   \"version\": \"3.13.0\"\n",
    "  }\n",
    " },\n",
    " \"nbformat\": 4,\n",
    " \"nbformat_minor\": 4\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6e7da6",
   "metadata": {},
   "source": [
    "## Cleaning examples\n",
    "Try a few example strings and compare before/after cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73949b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = [\n",
    "    '<p>Hello!!! Visit https://example.com for more üòä</p>',\n",
    "    'Contact us at info@example.com or call +1 (555) 123-4567',\n",
    "    'Repeated words words words words'\n",
    "]\n",
    "for s in samples:\n",
    "    print('RAW :', s)\n",
    "    print('CLEAN:', clean_text(s))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea68ff3",
   "metadata": {},
   "source": [
    "## Tokenization and lemmatization\n",
    "Show tokens and lemmas for a couple of rows from the generated dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db70b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "row = df.loc[0, 'text']\n",
    "print('Original:', row)\n",
    "cleaned = clean_text(row)\n",
    "print('Cleaned:', cleaned)\n",
    "print('Tokens:', tokenize_and_lemmatize(cleaned))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
